@article{nessler,
    doi = {10.1371/journal.pcbi.1003037},
    author = {Nessler, Bernhard AND Pfeiffer, Michael AND Buesing, Lars AND Maass, Wolfgang},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity},
    year = {2013},
    month = {04},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pcbi.1003037},
    pages = {1-30},
    number = {4},

}

@article{HierachicalBayesVisualCortex,
 doi = {doi: 10.1364/josaa.20.001434},
    author = {Lee TS, Mumford D.},
    journal = {J Opt Soc Am A Opt Image Sci Vis},
    title = {Hierarchical Bayesian inference in the visual cortex},
    year = {2003},
    month = {07},
}

@article{neuralSubstrate,
 doi = {doi: 10.1038/nn.4390},
    author = {Funamizu Akihiro, Kuhn Bernd, Doya Kenji},
    journal = {Nature Neuroscience},
    volume = {19},
    title = {Neural substrate of dynamic Bayesian inference in the cerebral cortex},
    year = {2016},
    month = {12},
}

@ARTICLE{anatomyOfInference,
  
AUTHOR={Parr, Thomas and Friston, Karl J.},   
	 
TITLE={The Anatomy of Inference: Generative Models and Brain Structure},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={12},           
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2018.00090},       
	
DOI={10.3389/fncom.2018.00090},      
	
ISSN={1662-5188},   
   
ABSTRACT={To infer the causes of its sensations, the brain must call on a generative (predictive) model. This necessitates passing local messages between populations of neurons to update beliefs about hidden variables in the world beyond its sensory samples. It also entails inferences about how we will act. Active inference is a principled framework that frames perception and action as approximate Bayesian inference. This has been successful in accounting for a wide range of physiological and behavioral phenomena. Recently, a process theory has emerged that attempts to relate inferences to their neurobiological substrates. In this paper, we review and develop the anatomical aspects of this process theory. We argue that the form of the generative models required for inference constrains the way in which brain regions connect to one another. Specifically, neuronal populations representing beliefs about a variable must receive input from populations representing the Markov blanket of that variable. We illustrate this idea in four different domains: perception, planning, attention, and movement. In doing so, we attempt to show how appealing to generative models enables us to account for anatomical brain architectures. Ultimately, committing to an anatomical theory of inference ensures we can form empirical hypotheses that can be tested using neuroimaging, neuropsychological, and electrophysiological experiments.}
}

@ARTICLE{nesslerClone,
  author={Guo, Shangqi and Yu, Zhaofei and Deng, Fei and Hu, Xiaolin and Chen, Feng},
  journal={IEEE Transactions on Cybernetics}, 
  title={Hierarchical Bayesian Inference and Learning in Spiking Neural Networks}, 
  year={2019},
  volume={49},
  number={1},
  pages={133-145},
  keywords={Bayes methods;Integrated circuit modeling;Neurons;Computational modeling;Biological system modeling;Hidden Markov models;Biological neural networks;Hierarchical Bayesian model;mean field theory;spike-timing-dependent plasticity (STDP);spiking neural network;variational expectation maximization;winner-takes-all (WTA) circuits},
  doi={10.1109/TCYB.2017.2768554}
  }
  
@ARTICLE{STDPDAN,
  author={Dan Yang, Poo Mu-Ming},
  journal={Neuron}, 
  title={Spike timing-dependent plasticity of neural circuits}, 
  year={2004},
  volume={44},
  pages={23-30},
  doi={10.1016/j.neuron.2004.09.007}
  }
  
@article{STDPFELDMAN,
title = {The Spike-Timing Dependence of Plasticity},
journal = {Neuron},
volume = {75},
number = {4},
pages = {556-571},
year = {2012},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2012.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0896627312007039},
author = {Daniel E. Feldman},
abstract = {In spike-timing-dependent plasticity (STDP), the order and precise temporal interval between presynaptic and postsynaptic spikes determine the sign and magnitude of long-term potentiation (LTP) or depression (LTD). STDP is widely utilized in models of circuit-level plasticity, development, and learning. However, spike timing is just one of several factors (including firing rate, synaptic cooperativity, and depolarization) that govern plasticity induction, and its relative importance varies across synapses and activity regimes. This review summarizes this broader view of plasticity, including the forms and cellular mechanisms for the spike-timing dependence of plasticity, and, the evidence that spike timing is an important determinant of plasticity in vivo.}
}

@BOOK{SpikingNeuronModelsBook, place={Cambridge}, title={Spiking Neuron Models: Single Neurons, Populations, Plasticity}, publisher={Cambridge University Press}, author={Gerstner, Wulfram and Kistler, Werner M.}, year={2002}}


@ARTICLE{softWTA,
  author={Rodney J Douglas, Kevan A C Martin},
  journal={Annual review of neuroscience}, 
  title={Neuronal circuits of the neocortex}, 
  year={2004},
  volume={27},
  pages={419-451},
  doi={10.1146/annurev.neuro.27.070203.144152}
  }
  
  @article{WTAPower,
    author = {Maass, Wolfgang},
    title = "{On the Computational Power of Winner-Take-All}",
    journal = {Neural Computation},
    volume = {12},
    number = {11},
    pages = {2519-2535},
    year = {2000},
    month = {11},
    abstract = "{This article initiates a rigorous theoretical analysis of the computational power of circuits that employ modules for computing winner-take-all. Computational models that involve competitive stages have so far been neglected in computational complexity theory, although they are widely used in computational brain models, artificial neural networks, and analog VLSI. Our theoretical analysis shows that winner-take-all is a surprisingly powerful computational module in comparison with threshold gates (also referred to as McCulloch-Pitts neurons) and sigmoidal gates. We prove an optimal quadratic lower bound for computing winner-takeall in any feedforward circuit consisting of threshold gates. In addition we show that arbitrary continuous functions can be approximated by circuits employing a single soft winner-take-all gate as their only nonlinear operation.Our theoretical analysis also provides answers to two basic questions raised by neurophysiologists in view of the well-known asymmetry between excitatory and inhibitory connections in cortical circuits: how much computational power of neural networks is lost if only positive weights are employed in weighted sums and how much adaptive capability is lost if only the positive weights are subject to plasticity.}",
    issn = {0899-7667},
    doi = {10.1162/089976600300014827},
    url = {https://doi.org/10.1162/089976600300014827},
    eprint = {https://direct.mit.edu/neco/article-pdf/12/11/2519/814328/089976600300014827.pdf},
}

@book{handbookWTA,
Author = {Arbib, Michael A.},
title = {The handbook of brain theory and neural networks 2nd Edition},
year = {2003},
isbn = { 0–262–01197–2},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
pages = {1228 - 1231}
}


@ARTICLE{hierarchicalBrain,
  
AUTHOR={Meunier, David and Lambiotte, Renaud and Fornito, Alex and Ersche, Karen and Bullmore, Edward},   
	 
TITLE={Hierarchical modularity in human brain functional networks},      
	
JOURNAL={Frontiers in Neuroinformatics},      
	
VOLUME={3},           
	
YEAR={2009},      
	  
URL={https://www.frontiersin.org/articles/10.3389/neuro.11.037.2009},       
	
DOI={10.3389/neuro.11.037.2009},      
	
ISSN={1662-5196},   
   
ABSTRACT={The idea that complex systems have a hierarchical modular organization originated in the early 1960s and has recently attracted fresh support from quantitative studies of large scale, real-life networks. Here we investigate the hierarchical modular (or “modules-within-modules”) decomposition of human brain functional networks, measured using functional magnetic resonance imaging in 18 healthy volunteers under no-task or resting conditions. We used a customized template to extract networks with more than 1800 regional nodes, and we applied a fast algorithm to identify nested modular structure at several hierarchical levels. We used mutual information, 0 < I < 1, to estimate the similarity of community structure of networks in different subjects, and to identify the individual network that is most representative of the group. Results show that human brain functional networks have a hierarchical modular organization with a fair degree of similarity between subjects, I = 0.63. The largest five modules at the highest level of the hierarchy were medial occipital, lateral occipital, central, parieto-frontal and fronto-temporal systems; occipital modules demonstrated less sub-modular organization than modules comprising regions of multimodal association cortex. Connector nodes and hubs, with a key role in inter-modular connectivity, were also concentrated in association cortical areas. We conclude that methods are available for hierarchical modular decomposition of large numbers of high resolution brain functional networks using computationally expedient algorithms. This could enable future investigations of Simon's original hypothesis that hierarchy or near-decomposability of physical symbol systems is a critical design feature for their fast adaptivity to changing environmental conditions.}
}

@book{basketCells,
editor = {Jones, EG; Hendry, SHC},
title = {Cerebral cortex: cellular components of the cerebral cortex.},
chapter = { Basket cells },
year = {1984},
isbn = { },
publisher = {ew York: Plenum Press},
pages = {309 – 334},
}

@article{chandelierCells,
title = {Electrophysiological classes of neocortical neurons},
journal = {Neural Networks},
volume = {17},
number = {5},
pages = {633-646},
year = {2004},
note = {Vision and Brain},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2004.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608004000863},
author = {Diego Contreras},
keywords = {Visual cortex, Intrinsic properties, Intracellular, Emergent behavior, Neuronal assemblies},
abstract = {Neocortical network behavior and neocortical function emerge from synaptic interactions among neurons with specific electrophysiological and morphological characteristics. The intrinsic electrophysiological properties of neurons define their firing patterns and their input–output functions with critical consequences for their functional properties within the network. Understanding the role played by the active non-linear properties caused by ionic conductances distributed in the soma and the dendrites is a critical step towards understanding cortical function. Here I present a brief description of electrophysiological and morphological characteristics of neocortical cells that allow their classification in categories. I review some examples of differences in functional properties among different electrophysiological cell classes in the visual cortex, as well as the role played by specific ionic conductances in defining firing and accommodation properties of neocortical neurons.}
}

@book{visualCortexBook,
author = {Huff T, Mahabadi N, Tadi P.},
title = {StatPearls [Internet]},
chapter = {Neuroanatomy, Visual Cortex.},
year = {2024},
publisher = {StatPearls Publishing},
link = {https://www.ncbi.nlm.nih.gov/books/NBK482504/},
}

@book{complexCellsIntegrated,
author = {Palmer, Stephen},
year = {1999},
month = {01},
pages = {153},
title = {Vision Science: From Photons to Phenomenology},
volume = {1}
}

@article{complexCellsFaces,
title = {Neural mechanisms of object recognition},
journal = {Current Opinion in Neurobiology},
volume = {12},
number = {2},
pages = {162-168},
year = {2002},
issn = {0959-4388},
doi = {https://doi.org/10.1016/S0959-4388(02)00304-5},
url = {https://www.sciencedirect.com/science/article/pii/S0959438802003045},
author = {Maximilian Riesenhuber and Tomaso Poggio},
keywords = {object recognition, computation, model, inferotemporal cortex, identification, categorization, fMRI, invariance, selectivity, feature},
abstract = {Single-unit recordings from behaving monkeys and human functional magnetic resonance imaging studies have continued to provide a host of experimental data on the properties and mechanisms of object recognition in cortex. Recent advances in object recognition, spanning issues regarding invariance, selectivity, representation and levels of recognition have allowed us to propose a putative model of object recognition in cortex.}
}

@ARTICLE{neuralImplementationOfBayesionInferenceSensoryMotor,
  title     = "Neural implementation of Bayesian inference in a sensorimotor
               behavior",
  author    = "Darlington, Timothy R and Beck, Jeffrey M and Lisberger, Stephen
               G",
  abstract  = "Actions are guided by a Bayesian-like interaction between priors
               based on experience and current sensory evidence. Here we unveil
               a complete neural implementation of Bayesian-like behavior,
               including adaptation of a prior. We recorded the spiking of
               single neurons in the smooth eye-movement region of the frontal
               eye fields (FEFSEM), a region that is causally involved in
               smooth-pursuit eye movements. Monkeys tracked moving targets in
               contexts that set different priors for target speed. Before the
               onset of target motion, preparatory activity encodes and adapts
               in parallel with the behavioral adaptation of the prior. During
               the initiation of pursuit, FEFSEM output encodes a maximum a
               posteriori estimate of target speed based on a
               reliability-weighted combination of the prior and sensory
               evidence. FEFSEM responses during pursuit are sufficient both to
               adapt a prior that may be stored in FEFSEM and, through known
               downstream pathways, to cause Bayesian-like behavior in pursuit.",
  journal   = "Nat. Neurosci.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  21,
  number    =  10,
  pages     = "1442--1451",
  month     =  oct,
  year      =  2018,
  language  = "en"
}

@Article{probabilisticBrain,
author={Pouget, Alexandre
and Beck, Jeffrey M.
and Ma, Wei Ji
and Latham, Peter E.},
title={Probabilistic brains: knowns and unknowns},
journal={Nature Neuroscience},
year={2013},
month={Sep},
day={01},
volume={16},
number={9},
pages={1170-1178},
abstract={Computational neuroscientists have started to shed light on how probabilistic representations and computations might be implemented in neural circuits, and here the authors review the application of these theories thus far. They further discuss the challenges that will emerge as researchers start expanding their use to more sophisticated, real-life computations.},
issn={1546-1726},
doi={10.1038/nn.3495},
url={https://doi.org/10.1038/nn.3495}
}

@article{LearningInBiologicallyPlausibleSNN,
title = {A review of learning in biologically plausible spiking neural networks},
journal = {Neural Networks},
volume = {122},
pages = {253-272},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303181},
author = {Aboozar Taherkhani and Ammar Belatreche and Yuhua Li and Georgina Cosma and Liam P. Maguire and T.M. McGinnity},
keywords = {Spiking neural network (SNN), Learning, Synaptic plasticity},
abstract = {Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity, information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed.}
}