Functional networks of the human brain have a hierarchical modular organization. This means the execution of one specific task, for example vision, involves several modules which are interconnected  \citep{hierarchicalBrain}. Traditional views of the visual pathway of the brain hypothesized that the sensory input is passed on from lower to higher visual areas in a feed-forward system. However, \citet{HierachicalBayesVisualCortex} shows  neurophysiological experimental evidence that there is feedback from high-level to low-level areas of the visual cortex. That feedback information is thought to be "explaining away" information, or putting emphasis on specific information of the low-level area. Through this mechanism can attention be realized and different cortex areas are able to adopt the same interpretation of some input.
 
The brain needs to handle a high degree of uncertainty in sensory input. When it first receives input it does not know what the information could represent, thus not knowing which parts of the input are most relevant. Several experiments on animals show that the computation of sensory input that the cerebral cortex performs can be explained and modelled by Bayesian inference \citep{neuralSubstrate, HierachicalBayesVisualCortex, anatomyOfInference}. They hypothesize that the brain functions as a generative and probabilistic model to reach its conclusions. This means the brain expresses information via probability distributions, rather than utilizing static neural codes. Bayes theorem yields a posterior probability, by multiplying a likelihood with a prior probability. In the context of cortical computation one can postulate that the posterior is represented by the output of a neural network. The likelihood can be computed based on the sensory input to the network. The feedback from high-level to low-level cortical areas can be modelled as the prior probability \citep{nessler}.

There are various models for the computational dynamics of biological neurons \citep{SpikingNeuronModelsBook}. One model which is well supported are spiking neural networks. These models aim to resemble biological neural networks more closely than common artificial neural networks. They generate neuron spikes which increase the membrane potentials of other neurons they are connected to. The impact those neuron spikes have is often modelled via Spike Timing Dependent Plasticity which takes the exact timing of pre- and postsynaptic spikes into account. This form of plasticity agrees with biological experiments \citep{STDPFELDMAN, STDPDAN}. To select the output of a neural network the soft Winner-Take-All mechanism is well established. Neurons with this mechanism choose what information is forwarded vertically into another layer and inhibit their horizontal neighbours of the same layer.  This mechanism resembles the way biological pyramidal neurons function \citep{softWTA}. Additionally Winner-Take-All modules perform computationally efficient \citep{WTAPower}.

\citet{nessler} and \citet{nesslerClone} created Winner-Take-All spiking neural networks which perform Bayesian inference. Both of those works did not include feedback that comes from a network higher up in the hierarchy. As such feedback is proven to exist by experiments this thesis aims to expand the model. The hierarchical network model of \citet{nessler} is taken and expanded to simulate a spiking neural network that receives visual input and also feedback from another network higher up in the hierarchy.

In Chapter 2 the biological background for this thesis and the underlying hierarchical spiking Winner-Take-All network will be given. The hierarchical structure of the brain will be explained and further looked at via an example of hierarchy in the visual cortex. To better understand the example a brief overview of the visual cortex will be given first. An argument, supported by many experiments, for the probabilistic brain will be given. Finally synaptic plasticity and spiking neural networks will be explained.
The theoretical background, explaining the mathematical model used for the neural networks in this thesis, will be explained in Chapter 3. In it Bayesian inference will be explained and how it can be applied to the example of hierarchical feedback in the visual cortex. After that the network model used for the simulation of the experiments will be explained and all necessary equations will be provided. Finally the link between the spiking Winner-Take-All network model and Bayesian inference will be explained and shown.
The various performed experiments will be provided in Chapter 4. There were four different experiments that helped to analyse the network model and the aforementioned mathematical link between the model and Bayesian inference. The first experiment shows how the network can decide how to interpret ambiguous images with the help of the additional added feedback. In the second experiment the conditional probabilities of the input and prior neurons are calculated and from those probabilities the weights of the network are determined. This proves the link between the spiking Winner-Take-All network model and Bayesian inference experimentally. Then the optimal network hyperparameters are searched. Furthermore, this experiment shows output for which there is no visual input, owed to the feedback. In Experiment 3 the transferability of network hyperparameters to networks of different sizes will be tested. At last in Experiment 4 the optimal hyperparameters from Experiment 2 will be used to train weights and test how close to the optimal solution one can get by training the weights.
Finally, in Chapter 5, the results of the experiments will be summarized and their implications will be discussed.