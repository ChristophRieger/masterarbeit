Functional networks of the human brain have a hierarchical modular organization. This means the execution of one specific task, for example vision, involves several modules which are interconnected  \citep{hierarchicalBrain}. Traditional views of the visual pathway of the brain hypothesized that the sensory input is passed on from lower to higher visual areas in a feed-forward system. However, \citet{HierachicalBayesVisualCortex} shows  neurophysiological experimental evidence that there is feedback from high-level to low-level areas of the visual cortex. That feedback information is thought to be "explaining away" information, or putting emphasis on specific information of the low-level area. Through this mechanism, attention can be realized and different cortex areas are able to adopt the same interpretation of  input.
 
The brain needs to handle a high degree of uncertainty in sensory input. When it first receives input it does not know what the information could represent, thus not knowing which parts of the input are most relevant. Several experiments on animals show that the computation of sensory input by the cerebral cortex can be explained and modelled by Bayesian inference \citep{neuralSubstrate, HierachicalBayesVisualCortex, anatomyOfInference}. They hypothesize that the brain functions in a generative and probabilistic way to reach its conclusions. This implies the brain expresses information via probability distributions, rather than utilizing static neural codes. 
Bayes theorem yields a posterior probability, by multiplying a likelihood with a prior probability. In cortical computation the posterior is  represented by the output of a neural network. The likelihood can be computed based on the sensory input to the network. The feedback from high-level to low-level cortical areas can be modelled as the prior probability \citep{nessler}.

There are various models for the computational dynamics of biological neurons \citep{SpikingNeuronModelsBook}. One model, which is well supported, is the spiking neural network. This model aims to resemble biological neural networks more closely than common artificial neural networks. It generates neuron spikes which increase the membrane potentials of other neurons they are connected to. The impact those neuron spikes have is often modelled via Spike Timing Dependent Plasticity (STDP) which takes the exact timing of pre- and postsynaptic spikes into account. This form of plasticity agrees with biological experiments \citep{STDPFELDMAN, STDPDAN}. To select the output of a neural network the soft Winner-Take-All mechanism is well established. Neurons with this mechanism choose what information is forwarded vertically into another layer and inhibit their horizontal neighbours of the same layer. This mechanism resembles the way biological pyramidal neurons function \citep{softWTA}.

\citet{nessler} and \citet{nesslerClone} created spiking Winner-Take-All  neural networks which perform Bayesian inference. Both of those works did not include feedback that comes from a network higher up in the hierarchy. As such feedback is proven to exist by experiments this thesis aims to expand the model. As a base, the hierarchical network model of \citet{nessler} is used. This network is further expanded to simulate a spiking neural network that receives visual input and also feedback from another network higher up in the hierarchy.

In Chapter 2, the biological background of this thesis is given. The hierarchical structure of the brain is explained and further looked at via an example of hierarchy in the visual cortex. To better understand the example, a brief overview of the visual cortex is given first. An argument, supported by experiments, for the probabilistic brain is given. This argument is essential for this thesis, as the network model is of probabilistic nature. Finally synaptic plasticity and spiking neural networks are explained.
The theoretical background, explaining the mathematical model used for the neural networks in this thesis, is explained in Chapter 3. In it, Bayesian inference is explained and how it can be applied to the example of hierarchical feedback in the visual cortex. After that, the network model  is explained and all necessary equations are provided. Finally, the link between the spiking Winner-Take-All network model and Bayesian inference is explained and shown.
The various performed experiments are provided in Chapter 4. There are four different experiments that helped to analyse the network model and the aforementioned mathematical link between the model and Bayesian inference. The first experiment shows how the network can decide how to interpret ambiguous images with the help of the additional added feedback. This behaviour of the network can be interpreted as it setting attention on some part of an image. Further, it is shown how the output neurons learned to respond to specific areas of images through unsupervised learning. In the second experiment the conditional probabilities of the input and prior neurons are calculated and from those probabilities the weights of the network are determined. This proves the link between the spiking Winner-Take-All network model and Bayesian inference experimentally. Then the best network hyperparameters are searched and their influence on the network analysed. Furthermore, this experiment shows output for which there is no visual input, owed to the feedback. This behaviour is similar to the behaviour of seeing illusory lines observed in the visual cortex by \citet{HierachicalBayesVisualCortex}. In Experiment 3 the transferability of network hyperparameters to networks of different sizes is tested. This proved impossible as the firing frequencies change the distribution of the output probabilities of the network. At last in Experiment 4 the best hyperparameters from Experiment 2 are used to train weights and to test how close to the analytical solution one can get by training the weights. Through that, problems in the training process of the network can be identified.
Finally, in Chapter 5, the results and implications of the experiments are discussed.