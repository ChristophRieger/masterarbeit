\section{Impact of the hyperparameters}
\label{section:impactHyper}

To further the understanding of the network model, each hyperparameter was analysed within Experiment 2.

\paragraph{$f_{input}$} $f_{input}$ controls how strongly the information of the input image is weighed. This means that by raising $f_{input}$ the impact of the active pixels increases, while the impact of the prior neuron decreases comparatively. This effect will be explained in the following paragraph about $f_{prior}$. Furthermore, by raising $f_{input}$ the membrane potentials of all output neurons are raised equally by the same percentage. However, this causes difficulties when trying to determine the optimal $f_{input}$. When raising $f_{input}$, it was also observed, that the posteriors of output classes adjacent to the active pixels decreased. This can be observed when comparing Tables \ref{tab:1D_42_0_15} and \ref{tab:1D_70_0_15}, where $f_{input}$ was raised from 42 Hz to 70 Hz. For example, when looking at Image 4 in the mentioned tables, it can be seen that for $f_{input} = 42 Hz$ the simulation output probability of $y_1$ is 0.245. This probability is due to the active pixel at position 2, which belongs to class 1 and 2 at the same time. $y_2$ has a higher simulation output probability of 0.553, as it has two active pixels within its active area. When increasing $f_{input}$ to 70 Hz one might expect the probabilities for $y_1$ and $y_2$ to rise. However, this does not happen, instead the simulation output probability for $y_1$ fell to 0.207, while for $y_2$ it rose to 0.685. It is assumed that this happened, because the membrane potentials of the output neurons were never normalized. As the input neurons spike more quickly, the membrane potential of $y_1$ rose by a smaller amount than the membrane potential of $y_2$, as there was one more active pixel within the active area of $y_2$. As given by Equation \ref{eqn:qk} $q_k$ depends exponentially on $u_k$. It is important to notice, that all $q_k$ together yield the discrete probability density function of the posterior. When $f_{input}$ is increased, all membrane potentials increase by the same percentage. Due to the exponential dependency of the probability density function on the membrane potentials, the different $q_k$, however, do not all change by the same percentage. Rather, every $q_k$ increases by a different percentage, the $q_k$ of $y_k$ with the most active pixels by the smallest amount and the $q_k$ of the $y_k$ with the least active pixels by the smallest percentage. However, this behaviour might be desired, as generating more input spikes corresponds stochastically to generating more samples from the likelihood. Taking more samples reduces the standard error of the posterior probability distribution, thus reducing the width of its probability density function around $q_k$, whose $y_k$ has the most active pixels. This means, that the more samples of the input the network takes, the more certain it becomes of the most likely option.

\paragraph{$f_{prior}$} When increasing the prior firing rate, the impact on the result of the prior neurons increased, while the impact of the input neurons decreased comparatively. This behaviour can be seen when comparing Figures \ref{fig:1D_88_440_4} and \ref{fig:1D_88_600_4}. They show the results for $f_{input} = 88 Hz$, $\tau_{decay} = 4 ms$ and $f_{prior} = 440$ and $600 Hz$ respectively. In Subfigure B6, the input image lay in the middle of the active areas of $y_1$ and $y_2$, while the prior neuron $z_4$ was active. When $f_{prior}$ was increased, the simulation output probability of $y_4$ rose, while all other probabilities fell. The opposite behaviour was observed when raising $f_{input}$ instead. The same changing of the probability density function of the posterior, as observed for $f_{input}$, was expected, as the prior neurons contribute to $u_k$ in the same way, as the input neurons. However, it was impossible to observe it separately, as increasing $f_{prior}$ inherently has the same effect. Per design, the prior neurons had the effect of narrowing, or widening the probability density function of the posterior, around the $q_k$, which is supported by the prior. This made the two effects indistinguishable from each other.

\paragraph{$\tau_{decay}$} $\tau_{decay}$ determines for how long and how strongly an input or prior spike contributes to the membrane potentials of the output neurons. The bigger $\tau_{decay}$ is, the smaller $f_{input}$ and $f_{prior}$ need to be, to minimize the Kullback-Leibler divergence. This is represented by the final hyperparameters for $\tau_{decay}$ of 4 and 15 ms. While for $\tau_{decay} = 15 ms$ the best input firing rate was 42 Hz and the prior firing rate was 222 Hz, for $\tau_{decay} = 4 ms$ they had to be raised to 98 and 440 Hz to minimize the Kullback-Leibler divergence.

\section{Basis of the network model}
\label{basisOfModel}
In this thesis a hierarchical spiking Winner-Take-All neural network model, which was based on \citet{nessler} and expanded by a neuron layer of prior neurons was used. The first hypothesis the network model is based on, is that $q_k$ is equal to the posterior probability $P(Y = k|X, Z)$. This relation was explained in Section \ref{linkNetworkBayes} and found conclusive. This model is also based on the hypothesis, that the weights of the network, due to STDP-induced changes, converge stochastically to the log of the conditional probability, that a presynaptic neuron has fired shortly before the postsynaptic neuron. This relation, given in Equation \ref{eqn:weightProbLink}, was proven in Experiment 2. The likelihood $P(X=x|Y=k)$, the prior $P(Y=k|Z=j)$ and the posterior $P(Y = k|X, Z)$ were analytically calculated. Then the input weights were derived from the likelihood and the prior weights were derived from the prior. Next, the network was simulated and its output was used to calculate the posterior of the simulation. The Kullback-Leibler divergence between the analytically calculated posterior and the posterior of the simulation was calculated, to evaluate how closely the simulation was able to approximate the optimal analytical solution. The results of the best hyperparameter set are given in Figure \ref{fig:1D_98_440_4} and Table \ref{tab:1D_98_440_4}. The simulation approximated the analytical solution closely and it can be concluded, that the hypothesis of Equation \ref{eqn:weightProbLink} was correct. However, it was not possible to tune the network to perfectly match the analytical solution. One difficulty when tuning the network was, that increasing or decreasing $f_{input}$ had an unexpected effect of changing the probability density function of the posterior. This effect was explained in Section \ref{section:impactHyper}. However, it is unclear why the analytic posterior could not be approximated closer, as the impacts of $f_{input}$, as well as $f_{prior}$ seemed correct. It might be possible, that varying the hyperparameter $\sigma$ would improve the training result.

\section{Reproducing behaviours of the visual cortex}
The experiments yielded evidence, that a spiking Winner-Take-All neural network can recreate behaviours, that were observed in the visual cortex. Neural feedback is most commonly associated with attention. Such attention behaviour could be shown in Experiment 1, where ambiguous cross images were shown to the network. Without the feedback and with perfectly trained output neurons the network would not have known, which part of the cross image to focus on. In reality, the output neurons ended up with different sized active areas and unequal relative activities. Due to that, when both parts of the cross were exactly in the middle of the theoretical centers of two output neurons, one output neuron was more active than the other. The network put its attention on either the horizontal or vertical part of the cross, even with inactive prior neurons. However, this unequal learning of the output neurons was no disturbance, as the feedback of the prior neurons could offset those errors. If the prior activity was set to "horizontal", the corresponding output neuron was always more active, than its vertical counterpart, even if the vertical neuron had a higher relative activity without the prior. The impact of the prior neurons was interpreted as the network putting attention on the corresponding part of the cross.

As shown by \citet{HierachicalBayesVisualCortex}, feedback from the inferior temporal cortex to V1 is able to convince V1 into "seeing" illusory lines. This behaviour was reproduced in Experiment 2. Figures \ref{fig:1D_88_0_4} and \ref{fig:1D_88_440_4} show the validation results of the same network with the same hyperparameter set, except once with disabled prior neurons and once with enabled ones. The sixth input image has active pixels at positions 1, 2 and 3. Thus, it lies in the middle between classes 1 and 2. When looking at Figure \ref{fig:1D_88_0_4}A6 and B6 it can be seen that with disabled prior neurons most of the output activity originates from $y_1$ and $y_2$, the output of the network  matches the visual input. In Figure \ref{fig:1D_88_440_4} with a prior that signals class 4 in Subfigures A6 and B6 it can be seen that the output probabilities shifted. The most active output neuron now is $y_4$, while $y_1$ and $y_2$ have smaller output probabilities. $y_4$, being the most active neuron, contradicts the visual input, but it is enforced by the prior information it receives. This feedback induced belief matched the experimental evidence of the visual cortex.

\section{Reusing hyperparameters for networks of different size}
The possibility to reuse a hyperparameter set for a network of different size was examined in Experiment 3. In this experiment the number of input neurons, the prior firing frequency and the size of the input images was doubled, compared to Experiment 2. Then the weights were re-calculated, according to the bigger size. When comparing Figures \ref{fig:1D_98_440_4} and \ref{fig:doubleSize_98_880_4} it can be seen, that the analysis output probabilities stayed the same. This was expected, as by doubling the pixels of the input images the information within them did not change. The areas corresponding to an output class doubled in size, as did the active pixels shown in black. The Kullback-Leibler divergence of this experiment was 0.2392 compared to 0.0101 of the original network. 
Furthermore, in Experiment 1 the best value of c was determined as 20, while in Experiment 4 it was 3. This indicates, that the best value of c is dependent on the network's size, $f_{input}$, $f_{prior}$ and $\tau_{decay}$. It was suspected that c converges towards a value, which ensures that the potentiation term of $\Delta w$ is in the right proportion to the depression term.
This clearly showed, that the hyperparameters could not simply be reused for a network of different size. One possible reason for this could be the, discussed in Section \ref{section:impactHyper}, effect of different $f_{input}$ changing the probability density function of $q_k$. By doubling the number of input neurons, twice as many samples were pulled from the input probability distribution, which made the network prefer output classes with more active pixels within their active areas over output classes with fewer active pixels. Furthermore, the impact of the prior neurons was too high, as seen in Figure \ref{fig:doubleSize_98_880_4}B6. As discussed in Section \ref{section:impactHyper}, the exponential dependency of $q_k$ on $u_k$ might have caused this increased impact of the prior neurons. For each validation image only one of the four output neurons received EPSPs from the active prior neuron, when noise is disregarded. By raising its $u_k$ to the power of e, its proportion to the other membrane potentials got bigger, than before the doubling of $f_{prior}$. This increased impact of the prior neurons is suspected to be the main reason why the transfer of the hyperparameters performed poorly.

\section{Training weights with predetermined hyperparameters}
In Experiment 4 the best hyperparameter set of Experiment 2 was used to train the input and prior weights of the network. The goal of this was to determine how well the network could approximate the optimal weights and the posterior. Only the weight shifting hyperparameter c was varied, to minimize the Kullback-Leibler divergence. With the best value of $c = 3$ the Kullback-Leibler divergence was 0.0342 compared to 0.0101 of the network with the calculated weights from Experiment 2. When comparing the likelihood and prior probability matrices in Figure \ref{fig:1DTrainingC3} it can be seen that the network learned the correct discrimination function. However, the likelihood values were too small, while the prior values were too big. This observation explains why further raising the value of c did not lower the Kullback-Leibler divergence, as the weights of the network are proportional to the mentioned probabilities. As raising c increases both likelihood and prior probabilities, it is impossible to further increase the likelihood probabilities, while also decreasing the prior probabilities. By adding a second separate weight shifting hyperparameter, it would be possible to increase the values of the input weights and to decrease the values of the prior weights at the same time. This would improve the performance of the trained network and might also grant more precise control over the output of the network.

\section{Future work}
The narrowing and widening of the probability density function of the posterior, when changing $f_{input}$, caused problems in this thesis and the model could potentially be improved by analysing it further. It was caused by the exponential dependency of $q_k$ on $u_k$, which seems intended and indicates that this behaviour was not caused by an implementation error of the model. It would be interesting to remove the exponential dependency to determine if the changing of the probability density function of the posterior disappears. Furthermore, it could be tested if the hyperparameters of the network would be reusable for networks of different sizes when the exponential dependency is removed.

The network architecture could be expanded by adding additional layers or networks, to increase the complexity of the experiment paradigms. For now, the prior neurons were told what feedback to give, but they could also receive input from another network. For example, a network modelling the inferior temporal cortex could deliver the prior, just as observed in biology. 
Another idea would be to add a network, which models a different sensory part of the brain. One could utilize audio signals, to train a second network, which then provides context about different environments the visual input might be coming from. For example, when looking at an ambiguous image, that might represent a human face, hearing human language might enforce that belief. When hearing bird chatter and the rustling of leaves, however, it might change the belief into seeing just parts of a tree, that look similar to a face.  
In the brain, the inferior temporal cortex receives feedback on how its own feedback impacted V1 via the visual pathway. This part of the feedback loop was neglected in the network architecture. The prior neurons could be fed the output of this thesis' network. Through that, the feedback loop would be complete.

Experiment 4 showed problems, that were caused by having only a single weight shifting hyperparameter for both input and prior weights. A second separate weight shifting parameter could be implemented, to quickly resolve this problem. A more sophisticated solution could be to design a variable weight shifting parameter, that depends on characteristics of the network and its neurons. The experiments suggested, that c depends on the network's size, $f_{input}$, $f_{prior}$ and $\tau_{decay}$. Thus, a variable weight shifting parameter could be designed, which depends on these hyperparameters.

\citet{triplets} reported, that pair-based STDP was not sufficient to explain synaptic changes, triggered by triplet or quadruplet spikes, observed in neurons. They showed that a triplet learning rule was able to approximate experimental data better. As the training in Experiment 4 prove imperfect, it could probably be improved, by using a more complex learning rule.

This thesis provided insight about the hierarchical spiking Winner-Take-All network model. The network from \citet{nessler} was successfully used as a template and was expanded by a layer, representing a-priori information. It was demonstrated, that the model was able to reproduce behaviours like attention and changing of belief through feedback. Through the insights of the experiments, several ideas of how to analyse and improve the model further were found.

